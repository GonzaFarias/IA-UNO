# -*- coding: utf-8 -*-
"""TF_CLBI (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qIVIb6dQDfksmJmMd3rcuJD0WEzc3iPU
"""

!pip install wandb -qU

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import wandb
from tensorflow import keras
from imblearn.over_sampling import SMOTE

# Librerías adicionales
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import random

wandb.login()

#IMPORTAMOS LOS DATOS (MODIFICAR RUTA)
df = pd.read_csv("/content/diabetes.csv", delimiter=",")

df

import seaborn as sns
import matplotlib.pyplot as plt

# Crear un boxplot para cada columna numérica
for column in df.select_dtypes(include=[np.number]).columns:
    plt.figure(figsize=(10, 5))
    sns.boxplot(df[column])
    plt.title(f'Boxplot de {column}')
    plt.show()

import numpy as np

# Definir función para eliminar outliers usando desviación estándar
def remove_outliers_std(df, num_std=3):
    # Calcular la media y la desviación estándar para cada columna
    mean = df.mean()
    std = df.std()

    # Filtrar los outliers: Mantener solo los valores que están dentro del rango de ±num_std desviaciones estándar
    df_filtered = df[(np.abs((df - mean) / std) < num_std).all(axis=1)]
    return df_filtered

# Aplicar la función a tu dataframe
df_sin_outliers_std = remove_outliers_std(df)

# Mostrar los resultados
print(f"Número de filas antes de eliminar outliers: {df.shape[0]}")
print(f"Número de filas después de eliminar outliers: {df_sin_outliers_std.shape[0]}")

import seaborn as sns
import matplotlib.pyplot as plt

# Calcular la matriz de correlación, incluye todas las columnas numéricas
correlation_matrix = df.corr()

# Crear el mapa de calor (heatmap) con todas las columnas, incluida la target
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Mapa de correlación (incluyendo la columna target)')
plt.show()

df.hist(bins=15, figsize=(15, 10), edgecolor='black')
plt.suptitle('Distribución de todas las características', size=16)
plt.show()

# Diagrama de dispersión de pares entre todas las variables
sns.pairplot(df, hue='Outcome', diag_kind='hist')  # Cambia 'target' por el nombre de tu columna objetivo
plt.suptitle('Relación entre las características', size=16)
plt.show()

# Asumimos que la última columna es la etiqueta binaria
X = df.iloc[:, :-1].values  # Todas las columnas menos la última (características)
y = df.iloc[:, -1].values   # Última columna (etiquetas)



# Dividimos el dataset en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Aplicar MinMaxScaler para escalar los datos entre 0 y 1
#scaler = MinMaxScaler()
#X_train_scaled = scaler.fit_transform(X_train)
#X_test_scaled = scaler.transform(X_test)

#from sklearn.preprocessing import RobustScaler

#scaler = RobustScaler()
#X_train_scaled = scaler.fit_transform(X_train)
#X_test_scaled = scaler.transform(X_test)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Contar los valores de la columna 'tu_columna'
conteo = df['Outcome'].value_counts()

# Mostrar el conteo de 0 y 1
print(conteo)

# Aplicar SMOTE para balancear el dataset de entrenamiento
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Definir el número de experimentos simulados
total_runs = 5

# Ejecutar múltiples experimentos
for run in range(total_runs):
    # Iniciar un nuevo experimento en wandb
    wandb.init(
        project="prueba_diabetes",
        name=f"experiment_{run}",
        config={
            "learning_rate": 0.01,
            "architecture": "Simple Neural Network with SMOTE",
            "dataset": "Custom CSV",
            "epochs": 10,
            "dropout_rate": 0.3,
        }
    )

    config = wandb.config

# Crear el modelo secuencial de red neuronal
model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(X_train_resampled.shape[1],))) #capa culta 16 neuronas
model.add(Dropout(0.3))
model.add(Dense(8, activation='relu', input_shape=(X_train_resampled.shape[1],))) #capa culta 16 neuronas
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))  # Capa de salida (clasificación binaria)

# Compilar el modelo con un optimizador, función de pérdida y métrica
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

# Entrenar el modelo y registrar el historial
history = model.fit(X_train_resampled, y_train_resampled,
                    epochs=wandb.config.epochs,
                    validation_data=(X_test, y_test),
                    batch_size=32)

# Registrar los resultados en wandb
wandb.log({'loss': history.history['loss'][-1], 'val_loss': history.history['val_loss'][-1], 'accuracy': history.history['accuracy'][-1], 'val_accuracy': history.history['val_accuracy'][-1]})

# Finalizar el experimento en wandb
wandb.finish()